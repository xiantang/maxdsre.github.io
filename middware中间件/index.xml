<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Middware(中间件)s on xiantang</title>
    <link>https://xiantang.github.io/middware%E4%B8%AD%E9%97%B4%E4%BB%B6/</link>
    <description>Recent content in Middware(中间件)s on xiantang</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="https://xiantang.github.io/middware%E4%B8%AD%E9%97%B4%E4%BB%B6/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title></title>
      <link>https://xiantang.github.io/middware%E4%B8%AD%E9%97%B4%E4%BB%B6/kafka/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xiantang.github.io/middware%E4%B8%AD%E9%97%B4%E4%BB%B6/kafka/</guid>
      <description>kafka 基础知识
 Topic：特指Kafka处理的消息源的不同分类，其实也可以理解为对不同消息源的区分的一个标识； Partition：Topic物理上的分组，一个topic可以设置为多个partition，每个partition都是一个有序的队列，partition中的每条消息都会被分配一个有序的id（offset）； Message：消息，是通信的基本单位，每个producer可以向一个topic（主题）发送一些消息； Producers：消息和数据生产者，向Kafka的一个topic发送消息的过程叫做producers（producer可以选择向topic哪一个partition发送数据）。 Consumers：消息和数据消费者，接收topics并处理其发布的消息的过程叫做consumer，同一个topic的数据可以被多个consumer接收； Broker：缓存代理，Kafka集群中的一台或多台服务器统称为broker。  在调用conusmer API时，一般都会指定一个consumer group，该group订阅的topic的每一条消息都发送到这个group的某一台机器上。借用官网一张图来详细介绍一下这种情况，假如kafka集群有两台broker，集群上有一个topic，它有4个partition，partition 0和1在broker1上，partition 2和3在broker2上，这时有两个consumer group同时订阅这个topic，其中一个group有2个consumer，另一个consumer有4个consumer，则它们的订阅消息情况如下图所示：
consumerGroup
因为group A只有两个consumer，所以一个consumer会消费两个partition；而group B有4个consumer，一个consumer会去消费一个partition。这里要注意的是，kafka可以保证一个partition内的数据是有序的，所以group B中的consumer收到的数据是可以保证有序的，但是Group A中的consumer就无法保证了。
group读取topic，partition分配机制是：
 如果group中的consumer数小于topic中的partition数，那么group中的consumer就会消费多个partition； 如果group中的consumer数等于topic中的partition数，那么group中的一个consumer就会消费topic中的一个partition； 如果group中的consumer数大于topic中的partition数，那么group中就会有一部分的consumer处于空闲状态。  </description>
    </item>
    
    <item>
      <title></title>
      <link>https://xiantang.github.io/middware%E4%B8%AD%E9%97%B4%E4%BB%B6/limiterrater/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xiantang.github.io/middware%E4%B8%AD%E9%97%B4%E4%BB%B6/limiterrater/</guid>
      <description>QS 接口调用频率限制
背景
分析了一下之前 890 的事故，结合之前的代码逻辑聊一下吧.
因为我们服务端调用代码的逻辑为异步，所以在请求的过程中是没有阻塞的。
def usersList(projectId: Int, ai: String, groupId: String, field: String, attrList: Seq[String], start: Int, end: Int): Future[Seq[UserInfo]] = { var index = start val requests = new ArrayBuffer[GroupUsersRequest]() // todo: 一个登陆用户对应多个设备的情况下 total 小于实际设备数  while (index &amp;lt; end) { val request = GroupUsersRequest( ....... ) } // 同时向qs请求  Future.traverse(requests.toList) { request =&amp;gt; requestInsight(request) }.map(_.flatten) } 18 w 个用户数据 会被切分成为 180 个 1000为单位的查询请求，由于没有阻塞 所以每台机器会瞬间发送 45 个请求到qs，也就是几毫秒发 180 连接到 QS，最终这些请求会变成查询压力打到数据库，将数据打挂。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xiantang.github.io/middware%E4%B8%AD%E9%97%B4%E4%BB%B6/rabbitmq/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xiantang.github.io/middware%E4%B8%AD%E9%97%B4%E4%BB%B6/rabbitmq/</guid>
      <description>rabbitmq-server用来启动RabbitMQ服务器进程：
 # rabbitmq-server -detached
 </description>
    </item>
    
  </channel>
</rss>