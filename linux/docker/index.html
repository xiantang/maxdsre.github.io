<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>xiantang </title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.63.0-DEV" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    
    
      <link href="/dist/css/app.3f5912c237ddd38c8e76debe081c7ca7.css" rel="stylesheet">
    

    

    
      
    

    
    
    <meta property="og:title" content="" />
<meta property="og:description" content="1、编写Scrapy的配置文件 [scrapyd] eggs_dir = eggs logs_dir = logs items_dir = jobs_to_keep = 5 dbs_dir = dbs max_proc = 0 max_proc_per_cpu = 10 finished_to_keep = 100 poll_interval = 5.0 bind_address = 0.0.0.0 http_port = 6800 debug = off runner = scrapyd.runner application = scrapyd.app.application launcher = scrapyd.launcher.Launcher webroot = scrapyd.website.Root [services] schedule.json = scrapyd.webservice.Schedule cancel.json = scrapyd.webservice.Cancel addversion.json = scrapyd.webservice.AddVersion listprojects.json = scrapyd.webservice.ListProjects listversions.json = scrapyd.webservice.ListVersions listspiders.json = scrapyd.webservice.ListSpiders delproject.json = scrapyd." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://xiantang.github.io/linux/docker/" />

<meta itemprop="name" content="">
<meta itemprop="description" content="1、编写Scrapy的配置文件 [scrapyd] eggs_dir = eggs logs_dir = logs items_dir = jobs_to_keep = 5 dbs_dir = dbs max_proc = 0 max_proc_per_cpu = 10 finished_to_keep = 100 poll_interval = 5.0 bind_address = 0.0.0.0 http_port = 6800 debug = off runner = scrapyd.runner application = scrapyd.app.application launcher = scrapyd.launcher.Launcher webroot = scrapyd.website.Root [services] schedule.json = scrapyd.webservice.Schedule cancel.json = scrapyd.webservice.Cancel addversion.json = scrapyd.webservice.AddVersion listprojects.json = scrapyd.webservice.ListProjects listversions.json = scrapyd.webservice.ListVersions listspiders.json = scrapyd.webservice.ListSpiders delproject.json = scrapyd.">

<meta itemprop="wordCount" content="271">



<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content=""/>
<meta name="twitter:description" content="1、编写Scrapy的配置文件 [scrapyd] eggs_dir = eggs logs_dir = logs items_dir = jobs_to_keep = 5 dbs_dir = dbs max_proc = 0 max_proc_per_cpu = 10 finished_to_keep = 100 poll_interval = 5.0 bind_address = 0.0.0.0 http_port = 6800 debug = off runner = scrapyd.runner application = scrapyd.app.application launcher = scrapyd.launcher.Launcher webroot = scrapyd.website.Root [services] schedule.json = scrapyd.webservice.Schedule cancel.json = scrapyd.webservice.Cancel addversion.json = scrapyd.webservice.AddVersion listprojects.json = scrapyd.webservice.ListProjects listversions.json = scrapyd.webservice.ListVersions listspiders.json = scrapyd.webservice.ListSpiders delproject.json = scrapyd."/>

  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="https://xiantang.github.io/" class="f3 fw2 hover-white no-underline white-90 dib">
      xiantang
    </a>
    <div class="flex-l items-center">
      

      
      













    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">

    <header class="mt4 w-100">
      <p class="f6 b helvetica tracked">
          
        LINUXES
      </p>
      <h1 class="f1 athelas mb1"></h1>
      
      
      <time class="f6 mv4 dib tracked" datetime="0001-01-01T00:00:00Z">January 1, 0001</time>
      
      
    </header>

    <section class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l"><h1 id="1编写scrapy的配置文件">1、编写Scrapy的配置文件</h1>
<pre><code class="language-conf" data-lang="conf">[scrapyd]
eggs_dir    = eggs
logs_dir    = logs
items_dir   =
jobs_to_keep = 5
dbs_dir     = dbs
max_proc    = 0
max_proc_per_cpu = 10
finished_to_keep = 100
poll_interval = 5.0
bind_address = 0.0.0.0
http_port   = 6800
debug       = off
runner      = scrapyd.runner
application = scrapyd.app.application
launcher    = scrapyd.launcher.Launcher
webroot     = scrapyd.website.Root

[services]
schedule.json     = scrapyd.webservice.Schedule
cancel.json       = scrapyd.webservice.Cancel
addversion.json   = scrapyd.webservice.AddVersion
listprojects.json = scrapyd.webservice.ListProjects
listversions.json = scrapyd.webservice.ListVersions
listspiders.json  = scrapyd.webservice.ListSpiders
delproject.json   = scrapyd.webservice.DeleteProject
delversion.json   = scrapyd.webservice.DeleteVersion
listjobs.json     = scrapyd.webservice.ListJobs
daemonstatus.json = scrapyd.webservice.DaemonStatus

</code></pre><h3 id="新建requirements">新建requirements</h3>
<p><code>vim requirements.txt</code></p>
<h3 id="dockerfile">Dockerfile</h3>
<p><code>vi Dockerfile</code></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-dockerfile" data-lang="dockerfile"><span style="color:#66d9ef">FROM</span><span style="color:#e6db74"> python:3.5</span><span style="color:#960050;background-color:#1e0010">
</span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">ADD</span> . /code<span style="color:#960050;background-color:#1e0010">
</span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">WORKDIR</span><span style="color:#e6db74"> /code</span><span style="color:#960050;background-color:#1e0010">
</span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">RUN</span> pip install  -r ./requirements.txt<span style="color:#960050;background-color:#1e0010">
</span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">EXPOSE</span><span style="color:#e6db74"> 6800</span><span style="color:#960050;background-color:#1e0010">
</span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">COPY</span> ./scrapyd.conf /etc/scrapyd/<span style="color:#960050;background-color:#1e0010">
</span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">CMD</span> [<span style="color:#e6db74">&#34;scrapyd&#34;</span>]<span style="color:#960050;background-color:#1e0010">
</span></code></pre></div><h3 id="建立镜像">建立镜像</h3>
<p><code>docker build -t scrapyd:test .</code></p>
<h3 id="启动">启动</h3>
<p><code>docker run -d -p 6800:6800 scrapyd</code></p>
<h3 id="部署">部署</h3>
<p><code>scrapyd-deploy 0</code></p>
<h3 id="调度爬虫项目">调度爬虫项目</h3>
<p><code>curl http://111.231.255.225:6800/schedule.json -d project=jdcrawler -d spider=DetailSpider</code></p>
<h3 id="查看正在运行的容器">查看正在运行的容器</h3>
<p><code>docker ps</code></p>
<h3 id="进入容器">进入容器</h3>
<p><code>sudo docker exec -it 0bf7d9d4aa4f bash</code></p>
<h3 id="部署爬虫项目">部署爬虫项目</h3>
<p><code>curl http://111.231.255.225:6800/schedule.json -d project=jdcrawler -d spider=DetailSpider</code></p>
<h3 id="取消某个爬虫">取消某个爬虫</h3>
<p><code>curl http://xiantang.info:6800/cancel.json -d project=jdcrawler -d job=65edbff03cd911e9ae0f0242ac110002</code></p>
<ul>
<li>登录阿里云Docker Registry
<code>$ sudo docker login --username=战神皮皮迪 registry.cn-hangzhou.aliyuncs.com</code>
用于登录的用户名为阿里云账号全名，密码为开通服务时设置的密码。您可以在产品控制台首页修改登录密码。</li>
<li>从Registry中拉取镜像
<code>$ sudo docker pull registry.cn-hangzhou.aliyuncs.com/xiantang/xiantang:[镜像版本号]</code></li>
<li>将镜像推送到Registry</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">$ sudo docker login --username<span style="color:#f92672">=</span>战神皮皮迪 registry.cn-hangzhou.aliyuncs.com
$ sudo docker tag <span style="color:#f92672">[</span>ImageId<span style="color:#f92672">]</span> registry.cn-hangzhou.aliyuncs.com/xiantang/xiantang:<span style="color:#f92672">[</span>镜像版本号<span style="color:#f92672">]</span>
$ sudo docker push registry.cn-hangzhou.aliyuncs.com/xiantang/xiantan:<span style="color:#f92672">[</span>镜像版本号<span style="color:#f92672">]</span>
</code></pre></div><p>请根据实际镜像信息替换示例中的[ImageId]和[镜像版本号]参数。</p>
<h1 id="docker-常用命令">docker 常用命令</h1>
<p>查看所有正在运行容器
<code>docker ps </code></p>
<p>查看所有容器
<code>docker ps -a</code></p>
<p>关闭指定容器
<code>docker stop [containerId]</code></p>
<p>删除已经退出的镜像</p>
<p><code>docker rm -v $(docker ps -a -q -f status=exited)</code></p>
<h2 id="删除所有的镜像">删除所有的镜像</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">docker rmi <span style="color:#66d9ef">$(</span>docker images -q<span style="color:#66d9ef">)</span>
</code></pre></div><h1 id="ubuntu-1604-安装-docker">Ubuntu 16.04 安装 Docker</h1>
<p>1.选择国内的云服务商，这里选择阿里云为例
<code>curl -sSL http://acs-public-mirror.oss-cn-hangzhou.aliyuncs.com/docker-engine/internet | sh -</code></p>
<p>2.安装所需要的包
<code>sudo apt-get install linux-image-extra-$(uname -r) linux-image-extra-virtual</code></p>
<p>3.添加使用 HTTPS 传输的软件包以及 CA 证书</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo apt-get update      
sudo apt-get install apt-transport-https ca-certificates
</code></pre></div><p>4.添加GPG密钥
<code>sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D</code></p>
<p>5.添加软件源<code>echo &quot;deb https://apt.dockerproject.org/repo ubuntu-xenial main&quot; | sudo tee /etc/apt/sources.list.d/docker.list</code>
6.添加成功后更新软件包缓存<code>sudo apt-get update</code>
7.安装docker
<code>sudo apt-get install docker-engine</code>
8.启动 docker
<code> sudo systemctl enable docker</code>
<code>sudo systemctl start docker</code></p>
<ul class="pa0">
  
</ul>
<div class="mt6">
      
      
      </div>
    </section>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://xiantang.github.io/" >
    &copy;  xiantang 2020 
  </a>
    <div>












</div>
  </div>
</footer>

    

  <script src="/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
